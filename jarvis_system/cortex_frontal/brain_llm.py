import os
import time
import datetime
import re
from typing import Optional
from groq import Groq
import ollama

# Imports do Sistema
from jarvis_system.cortex_frontal.observability import JarvisLogger

log = JarvisLogger("CORTEX_BRAIN")

try:
    from jarvis_system.hipocampo.memoria import memoria
except ImportError:
    log.warning("Hipocampo (MemÃ³ria) nÃ£o encontrado ou falhou ao carregar.")
    memoria = None

class HybridBrain:
    def __init__(self):
        self.api_key = os.getenv("GROQ_API_KEY")
        self.model_cloud = os.getenv("JARVIS_MODEL_CLOUD", "llama-3.3-70b-versatile")
        self.model_local = os.getenv("JARVIS_MODEL_LOCAL", "qwen2:0.5b")
        
        self.client_groq: Optional[Groq] = None
        self._initialize_cloud()

    def _initialize_cloud(self):
        if self.api_key:
            try:
                self.client_groq = Groq(api_key=self.api_key)
                log.info(f"â˜ï¸ CÃ³rtex Nuvem Conectado: {self.model_cloud}")
            except Exception as e:
                log.error(f"âŒ Erro ao conectar Groq: {e}")
        else:
            log.warning("âš ï¸ Modo Offline ForÃ§ado (Sem API Key).")

    @property
    def _dynamic_system_prompt(self) -> str:
        now = datetime.datetime.now()
        data_str = now.strftime("%d/%m/%Y")
        hora_str = now.strftime("%H:%M")
        
        return (
            f"VocÃª Ã© J.A.R.V.I.S., uma IA avanÃ§ada de automaÃ§Ã£o e companhia. Data: {data_str}, Hora: {hora_str}.\n\n"
            "### DIRETRIZES MESTRAS:\n"
            "1. **AUTOMACAO**: Se o usuÃ¡rio pedir mÃºsica, abrir apps ou controle de PC, retorne APENAS um JSON.\n"
            "   - MÃºsica: `{\"ferramenta\": \"spotify\", \"comando\": \"...\"}`\n"
            "   - App: `{\"ferramenta\": \"sistema\", \"comando\": \"abrir ...\"}`\n\n"
            "2. **MEMÃ“RIA**: Se o usuÃ¡rio disser 'Aprenda que...', retorne JSON: `{\"ferramenta\": \"memoria_gravar\", \"dado\": \"...\"}`\n"
            "3. **CHAT**: Se for pergunta geral ('Quem Ã© vocÃª?', 'Piada', 'Sentido da vida'), RESPONDA COMO CHATBOT.\n"
            "   - Seja espirituoso, breve e Ãºtil. Personalidade: Jarvis do Homem de Ferro.\n\n"
            "### REGRAS ESPECÃFICAS:\n"
            "- Se o usuÃ¡rio falar apenas 'status', assuma que Ã© um check de sistema.\n"
            "- NÃ£o invente nomes de ferramentas (ex: sistema_ping nÃ£o existe).\n"
            "- Para perguntas sobre o usuÃ¡rio ('Quem sou eu?'), use o contexto fornecido."
        )

    def _verificar_intencao_forcada(self, texto: str) -> Optional[str]:
        """
        HeurÃ­stica: Intercepta comandos Ã³bvios antes de gastar IA.
        Isso ajuda a garantir que 'tocar coldplay' vÃ¡ para o Spotify.
        """
        t = texto.lower().strip()
        
        # Lista de verbos musicais que exigem busca
        verbos_busca = ["tocar", "ouvir", "bota", "reproduzir", "som de", "escutar"]
        
        for verbo in verbos_busca:
            if re.search(rf"\b{verbo}\s+.{{2,}}", t):
                # Se detectado, forÃ§amos o LLM a seguir este caminho
                return f"Comando de mÃºsica detectado: '{texto}'. AÃ§Ã£o esperada: spotify"

        return None

    def pensar(self, texto_usuario: str) -> str:
        start_time = time.time()
        
        # 1. RecuperaÃ§Ã£o de Contexto (RAG)
        contexto_rag = self._recuperar_memoria(texto_usuario)
        
        # 2. ReforÃ§o HeurÃ­stico
        dica_intencao = self._verificar_intencao_forcada(texto_usuario)
        
        # 3. Montagem do Prompt
        prompt_final = self._montar_prompt_usuario(texto_usuario, contexto_rag, dica_intencao)
        
        resposta = ""
        provider = "NENHUM"

        # 4. InferÃªncia
        if self.client_groq:
            try:
                resposta = self._inferencia_nuvem(prompt_final)
                provider = f"NUVEM ({self.model_cloud})"
            except Exception as e:
                log.warning(f"Falha na Nuvem: {e}. Tentando local...")
        
        if not resposta:
            try:
                resposta = self._inferencia_local(prompt_final)
                provider = f"LOCAL ({self.model_local})"
            except Exception as e:
                log.critical(f"Falha Cognitiva Total: {e}")
                return "Erro crÃ­tico no sistema."

        latency = time.time() - start_time
        log.info(f"ðŸ§  Pensamento: {latency:.2f}s via {provider}")
        return resposta

    def ensinar(self, fato: str):
        """MÃ©todo direto para gravar memÃ³ria sem passar pelo 'pensar'"""
        if not memoria: return "Erro: MemÃ³ria off."
        
        try:
            # Tenta encontrar o mÃ©todo correto dinamicamente
            if hasattr(memoria, "adicionar_memoria"):
                memoria.adicionar_memoria(fato)
            elif hasattr(memoria, "memorizar"):
                memoria.memorizar(fato)
            elif hasattr(memoria, "gravar"):
                memoria.gravar(fato)
            else:
                # Fallback genÃ©rico se o nome do mÃ©todo nÃ£o for Ã³bvio
                log.error(f"Interface de memÃ³ria incompatÃ­vel. MÃ©todos disponÃ­veis: {dir(memoria)}")
                return "Erro tÃ©cnico na memÃ³ria."
                
            return "MemÃ³ria gravada com sucesso."
            
        except Exception as e:
            log.error(f"Erro ao gravar memÃ³ria: {e}")
            return "Falha ao acessar banco de memÃ³ria."

    # --- AUXILIARES ---

    def _recuperar_memoria(self, query: str) -> str:
        if not memoria: return ""
        try:
            # Tenta buscar contexto relevante no ChromaDB
            dados = memoria.relembrar(query)
            if dados: return dados
        except: pass
        return ""

    def _montar_prompt_usuario(self, query: str, context: str, dica: str = None) -> str:
        reforco = ""
        if dica:
            reforco = f"INSTRUÃ‡ÃƒO DO SISTEMA: {dica}. OBEDEÃ‡A A ESTA CLASSIFICAÃ‡ÃƒO.\n"

        base = f"USUÃRIO: {query}"
        ctx = f"MEMÃ“RIA/CONTEXTO:\n{context}\n" if context else ""
        
        return f"{reforco}{ctx}---\n{base}"

    def _inferencia_nuvem(self, prompt: str) -> str:
        if not self.client_groq: raise Exception("Groq Off")
        chat = self.client_groq.chat.completions.create(
            model=self.model_cloud,
            messages=[
                {"role": "system", "content": self._dynamic_system_prompt},
                {"role": "user", "content": prompt}
            ],
            # Temperature ajustada: 0.3 permite criatividade no chat mas mantÃ©m rigor nos comandos
            temperature=0.3, 
            max_tokens=300,
            timeout=6.0
        )
        return chat.choices[0].message.content

    def _inferencia_local(self, prompt: str) -> str:
        response = ollama.chat(
            model=self.model_local,
            messages=[
                {"role": "system", "content": self._dynamic_system_prompt},
                {"role": "user", "content": prompt}
            ],
            options={"temperature": 0.3, "num_predict": 128}
        )
        return response['message']['content']

try:
    llm = HybridBrain()
except Exception as e:
    log.critical(f"FATAL: {e}")
    llm = None